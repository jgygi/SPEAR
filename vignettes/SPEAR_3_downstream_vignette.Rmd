---
title: "SPEAR: Downstream Analysis"
author: "Jeremy Gygi"
date: "1/26/21"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{SPEAR_3-downstream_analysis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

### This vignette will provide insight into downstream analysis using SPEAR:

1. Analyzing all SPEAR models (including SPEARgaussian)

2. Functions specific to SPEARordinal, SPEARbinomial, and SPEARcategorical models

##### Installing SPEAR:

Follow installation instructions in README (located [here](https://github.com/jgygi/SPEAR))

##### Required Libraries:

```{r message = FALSE, warning = FALSE}
# Required Packages for SPEAR:
library(SPEARcomplete)
library(glmnet)
library(parallel)
library(ordinalNet)

# Recommended Packages (for downstream analysis):
library(dplyr)
library(reshape2)
library(stringr)
library(ggplot2)
library(cowplot)
```


### 1) Analyzing all SPEAR models (including SPEARgaussian)

```{r}
# Load in the SPEARobject saved from running SPEAR:
file.path <- getwd()
SPEARobj <- readRDS(paste0(file.path, "/SPEAR_vignette_object.rds"))
names(SPEARobj)
```

The SPEAR object returned has 4 components:
1. fit (the cross-validated SPEAR object)
2. cv.eval (the results from evaluating the fit object)
3. params (the parameters used for this specific SPEAR object)
4. data (the data used to train the SPEAR models in the object)

*Variables used to reference results:*

+ $N$ - number of subjects [nrow($X$)]

+ $P$ - number of total features (across all omics) [ncol($X$)]

+ $D$ - number of omics datasets [length($X$)]

+ $G$ - number of groups of subjects (typically 1)

+ $Y$ - number of response vars [ncol($Y$)]

+ $K$ - number of factors SPEAR constructed

+ $W$ - number of weights used in CV

+ $F$ - number of folds used in CV



1. fit - the cross-validated SPEAR object

    + results
    
        + post_betas ($P$ x $K$ x $G$ x $W$)
        
        + post_bys ($K$ x $Y$ x $W$)
        
        + post_bxs ($K$ x $P$ x $W$)
        
        + post_pis ($P$ x $K$ x $W$)
        
        + post_selections ($P$ x $K$ x $W$)
        
        + interceptsX ($P$)
        
        + interceptsY ($Y$)
        
    + factors_coefs ($P$ x $K$ x $G$ x $F$ x $W$)
    
    + projection_coefs ($K$ x $Y$ x $F$ x $W$)
    
    + foldid ($N$)
    
2. cv.eval - results from evaluating 'fit'

    + projection_coefs ($K$ x $Y$ x $W$)
    
    + reg_coefs ($P$ x $G$ x $Y$ x $W$)
    
    + intercepts ($Y$)
    
    + cvm ($W$ x $Y$)
    
    + cvsd ($W$ x $Y$)
    
    + factor_contributions ($K$ x $Y$ x $W$)
    
    + Yhat.keep ($N$ x $Y$ x $W$)
    
3. params - the parameters used to generate 'fit' by SPEAR

    + weights ($W$) - values of '$w$' used in SPEAR
    
    + foldid ($N$) - integer of 1:$F$
    
    + seed
    
    + num_factors
    
    + max_iter
    
    + thres_elbo
    
    + thres_count
    
4. data - the data used by SPEAR

While it isn't necessary to extract these matrices directly from the model, it is useful to understand where the results are coming from. Most typical downstream analyses can be performed using the built-in SPEAR functions described below.

### Getting the best value of w (weight)

SPEAR generates factors based on a weight parameter, $w$. As SPEAR is reconstructing factors, it uses the weighted objective function below.

![SPEAR overview](vignette_weightexplanation.png){width=500px}

When $w\approx0$, it means that SPEAR will focus on generating factors that influence the response ($Y$). 

When $w\ge1$, it means that SPEAR is equally considering the relationship between the constructed factors and both the datasets ($X$) and the response ($Y$). 

This $w$ parameter is tuned automatically, meaning SPEAR will return factors constructed on the "best" $w$ (based on mean cross-validated prediction error from the training) by default. 

SPEAR will generate a different set of factors for each value supplied in the 'weights' argument (if no argument is supplied, will default to c(2, 1.8, 1.6, 1.4, 1.2, 1.0, 0.8, 0.6, 0.4, 0.2, 0.0).
```{r}
# Check values of 'w' (weights) used by SPEAR:
SPEARobj$params$weights
```

SPEAR uses the mean cross-validation error to determine which is the best $w$ to use.
```{r}
# Get best weights (w):
SPEAR.get_best_weights(SPEARobj)
```

We can plot the mean cross-validation errors to visualize the impact of $w$ on the data.

```{r fig.width = 7}
SPEAR.plot_cv_prediction_errors(SPEARobj, show.w.labels = TRUE, show.min.w.line = TRUE)
```


### Factor Contributions:

The goal of SPEAR is to reconstruct U (factors) that are found in both X (the omics data) and Y (the response). We can calculate the contribution of each of the Ku factors below to find which factors are relevant in predicting Y:

![SPEAR overview](vignette_spearoverview.png){width=600px}

```{r}
# Get factor contributions for each factor generated by SPEAR. Will return:
#   factor.contributions: the exact contribution amounts per factor
#   relevant.factors:     which factors are deemed "relevant" (contribution > threshold, .01 by default)
#   threshold:   the cutoff used to determine which factors are "relevant"
SPEAR.get_factor_contributions(SPEARobj, threshold = .01)
```

We can also plot the factor contributions for visualization:

```{r fig.width = 7}
# Plot factor contributions:
#     threshold: cutoff used to determine factor relevance
#     show.irrelevant: if FALSE, will only plot relevant factor contributions
#     show.labels: show numeric contributions in plot?
SPEAR.plot_factor_contributions(SPEARobj, threshold = .01, show.irrelevant = FALSE, show.labels = TRUE)
```

```{r fig.width = 7}
# As a quick aside, plotting functions all return ggplot grob objects, and can be easily edited as such:
plot <- SPEAR.plot_factor_contributions(SPEARobj, threshold = .01, show.irrelevant = FALSE, show.labels = TRUE)
plot + 
  scale_fill_distiller(palette = "Reds", direction = 1) +
  ggtitle("Custom Factor Contribution Plot")
```

### Cross-validation results:

Here, we can analyze the results from running the cross-validated version of SPEAR.

First, we check the prediction error:
```{r}
SPEAR.get_cv_loss(SPEARobj)
```

Plotting the cross-validation predictions:
```{r}
# Plot cv predictions for the best weight:
SPEAR.plot_cv_predictions(SPEARobj)
```

Finally, if we want to extract the SPEAR predictions, we use the functions below. Note that there are options for which weight to use (as a different value of 'w' means different factors, and different predictions). The SPEAR predictons to be used per response should be the weight with the lowest cvm (i.e. 'best' or 'overall')
```{r}
spear.predictions.best <- SPEAR.get_predictions(SPEARobj, w = "best")
spear.predictions.overall <- SPEAR.get_predictions(SPEARobj, w = "overall")

# We can also choose by hand the value of w trained by SPEAR:
spear.predictions_w2 <- SPEAR.get_predictions(SPEARobj, w = 2)

# If we try to use a weight that wasn't used by SPEAR (see SPEARobj$params$weights), it will use the closest weight instead:
spear.predictions_w1.5 <- SPEAR.get_predictions(SPEARobj, w = 1.5)

print(spear.predictions.best$GausResponse$predictions[1:10])
```

### Using Factor Scores

We can try to interpret our factors by looking at the distributions of factor scores. We can also split our subjects into groups (based on response, clinical data, etc.) and see if there are any major differences in factor scores between groups.

```{r fig.height=2.6, fig.width=9}
# First, set up a named vector called 'groups'. Set names(groups) to be the subject for each grouping variable:

# Example of setting up a continuous 'groups' vector:
# Use the response as the variable we want to plot:
groups <- SPEARobj$data$Y
# Set the names of the vector to be the samples they correspond to (make sure they are ordered correctly)
names(groups) <- rownames(SPEARobj$data$Y)


# Plot the factor scores:
SPEAR.plot_factor_scores(SPEARobj, groups = groups)
```

```{r fig.height=2.6, fig.width=9}
# Lets convert the continuous response into ordinal (bins)
# Use the response as the variable we want to plot:
groups <- round(SPEARobj$data$Y, 0)
groups <- factor(as.character(groups), levels = as.character(unique(sort(groups))))
# Set the names of the vector to be the samples they correspond to (make sure they are ordered correctly)
names(groups) <- rownames(SPEARobj$data$Y)


# Plot the factor scores:
SPEAR.plot_factor_scores(SPEARobj, groups = groups)
```

We can also plot a matrix grid of factor scores, with the diagonal representing the distributions of factor scores:

```{r fig.width = 9, fig.height = 7}
SPEAR.plot_factor_grid(SPEARobj, groups = groups)
```

We can choose to only plot a subset of factors: (i.e. the relevant factors)

```{r fig.width=9, fig.height = 7}
relevant.factors.to.Y <- which(SPEAR.get_factor_contributions(SPEARobj)$Y.relevant.factors == 1)
SPEAR.plot_factor_grid(SPEARobj, groups = groups, factors = relevant.factors.to.Y)
```

Finally, we can look into factor scores vs. the cv-predictions from SPEAR:

```{r fig.width = 9, fig.height = 4}
# This will plot two rows of plots:
# top row - Factor Scores (X axis) vs. SPEAR CV Predictions
# bottom row - Factor Scores (X axis) vs. True response (SPEARobj$data$Y)

f.scores <- SPEAR.get_factor_scores(SPEARobj)$GausResponse$factor.scores
spear.preds <- SPEAR.get_predictions(SPEARobj)
y <- SPEARobj$data$Y
df <- data.frame(Factor1 = f.scores[,1],
                 Factor2 = f.scores[,2],
                 Factor3 = f.scores[,3],
                 Factor4 = f.scores[,4],
                 Factor5 = f.scores[,5], # Assuming 5 factors were used
                 spear.preds = unname(spear.preds$GausResponse$predictions),
                 GausResponse = y)

plots <- list()
# Versus the predicted values from SPEAR:
for(k in 1:SPEARobj$params$num_factors){
  g <- ggplot(df) +
    geom_point(aes_string(x = paste0("Factor", k), y = "spear.preds")) +
    geom_smooth(aes_string(x = paste0("Factor", k), y = "spear.preds"), method = "lm", color = "red") +
    theme_bw()
  plots[[k]] <- g
}
# Versus the actual response data:
for(k in 1:SPEARobj$params$num_factors){
  g <- ggplot(df) +
    geom_point(aes_string(x = paste0("Factor", k), y = "GausResponse")) +
    geom_smooth(aes_string(x = paste0("Factor", k), y = "GausResponse"), method = "lm") +
    theme_bw()
  plots[[SPEARobj$params$num_factors + k]] <- g
}
cowplot::plot_grid(plotlist = plots, nrow = 2)
```



### Feature Inspection

Now that we have an idea about the factor scores themselves, we can try to figure out what features from X influence each factor. This can be done by inspecting the factor loadings matrix. This is a P x K matrix (P = total number of features from ALL X datasets, K = number of factors generated by SPEAR). Each feature is assigned a posterior selection probability from 0-1. From here, a cut-off (typically .5) can be made to select relevant features per factor (i.e. if probability >= .5).

```{r fig.width = 5, fig.height = 10}
# Plot all factor loadings
#     plot.irrelevant.factors - should factors that aren't relevant to predicting Y be shown?
SPEAR.plot_factor_loadings(SPEARobj)
```

By default, only the loadings associated with the response (Y) are shown. You can show all loadings with the `plot.irrelevant.factors` parameter:

```{r fig.width = 5, fig.height = 10}
SPEAR.plot_factor_loadings(SPEARobj, plot.irrelevant.factors = TRUE)
```

We can get the names of the features and probabilities for each factor with the function below. Data will be returned as a list:

Y - colnames(SPEARobj\$data\$Y)
    + Factors - names of factors found to be relevant for that response (i.e. Factor1, Factor2)
        + X - names of omics datasets that have at least one feature associated with the response
            + features - vector of relevant features (pulled from the column names of X passed into SPEAR)
            + probabilities - named list of posterior selection probabilities of features, names = features
            + coefficients - named list of factor coefficients for the features, names = features
            
The features are sorted by probability and then by coefficient magnitude.
            
```{r}
# Get the names of features contributing to relevant factors
#   cutoff - posterior probability selection cutoff, features with probabilities higher than 'cutoff' will be added to the list
#   threshold - minimum factor contribution to be deemed "relevant"
feature.list <- SPEAR.get_factor_features(SPEARobj, cutoff = .5, threshold = .01)
feature.list$GausResponse$Factor1$OmicsData1 # show relevant factors per response
```

You can see the distribution of probabilities and their associated coefficients with `plot_factor_coefficients`:

```{r fig.width = 8}
SPEAR.plot_factor_coefficients(SPEARobj)
```

You can also check the number of overlapping features per omic across factors:
```{r fig.width = 8}
SPEAR.plot_feature_overlap(SPEARobj)
```

### SPEAR predicting new (testing) data

If a testing dataset has been set aside from the data SPEAR was trained on, you can use a trained SPEAR model to predict responses for the new data. This testing data is required to have the same number of features (columns) in X as the training dataset.

```{r fig.width = 7}
# Below uses simulated data generated in the SPEAR_simulate_data.Rmd
path_to_sim_data <- getwd() # Replace with the path to the 'simulated_gaussian_data.rds' file
sim.data <- readRDS(paste0(path_to_sim_data, "/simulated_gaussian_data.rds"))


Xtest <- sim.data$data.te$xlist # replace with your testing data here
Ytest <- sim.data$data.te$Y     # replace with your testing data response if you have it here

# Make sure the dimensions of the test data match the dimensions of the training data
sapply(SPEARobj$data$xlist, ncol)
sapply(Xtest, ncol)

# Get predictions:
SPEARpreds <- SPEAR.get_predictions(SPEARobj, Xtest, scale.x = TRUE)
# Below is the same thing in case you want to work directly with the matrices:
#SPEARpreds <- scale(data.total$data.te$X) %*% SPEARobj$cv.eval$reg_coefs[,1,,which.min(SPEARobj$cv.eval$cvm)] + SPEARobj$cv.eval$intercepts[[1]][which.min(SPEARobj$cv.eval$cvm),]
SPEARpreds <- SPEARpreds$GausResponse$predictions
truth <- Ytest
# Plot predictions:
g <- ggplot() +
  geom_point(aes(x = SPEARpreds, y = truth)) +
  geom_smooth(aes(x = SPEARpreds, y = truth), method = "lm") +
  xlab("SPEAR Predictions") +
  ylab("True Response") +
  theme_bw()
g
```

### 2) Analyzing SPEARordinal, SPEARbinomial, and SPEARcategorical models

SPEAR will treat responses labeled as `"ordinal"`, `"binomial"`, and `"categorical"` (from the `family` parameter) differently. As such, you can extract more information, such as class probabilities, from these models.

```{r}
# Load in the SPEARobject saved from running SPEAR:
file.path <- getwd()
SPEARobj.ordinal <- readRDS(paste0(file.path, "/SPEAR_vignette_object_ordinal.rds"))
names(SPEARobj.ordinal)
print(SPEARobj.ordinal$params$family)
# For this example dataset, there are 5 classes:
print(table(SPEARobj.ordinal$data$Y))
```

##### Class Probabilities:

When using the `get_predictions` function, the probabilities of each class are also returned. This probabilites matrix has dimensions of (N.samples x N.classes).

```{r}
preds <- SPEAR.get_predictions(SPEARobj.ordinal)
# $probabilities is added as a nested list
names(preds$OrdinalResponse)
# $probabilities has dimensions: (N.samples x N.classes) (rows x cols)
print(dim(preds$OrdinalResponse$probabilities))
```

Once the probabilities are extracted, they can be used to see how likely it is that a sample is labeled a certain class:
```{r}
subject.probabilities <- preds$OrdinalResponse$probabilities[1,]
ggplot() +
  geom_point(aes(x = (1:length(subject.probabilities)-1), y = subject.probabilities)) +
  geom_line(aes(x = (1:length(subject.probabilities)-1), y = subject.probabilities)) +
  xlab("Class") +
  ylab("Probabillity of Class Assignment") +
  theme_minimal()
```

For the above subject, the class with the highest probability is **2**, so that is the prediction that SPEARordinal would make. However, there is still a high probability that the subject could be of class **3**. Returning the probabilities helps visualize when the model is uncertain.

Let's print the true class of this first subject:
```{r}
print(SPEARobj$data$Y[1,])
```

Plot the distribution of class sample probabilities with `SPEAR.plot_ordinal_class_probabilities`

```{r fig.height = 3, fig.width = 10}
SPEAR.plot_ordinal_class_probabilities(SPEARobj.ordinal)
```


```{r fig.height = 3, fig.width = 6}
SPEAR.plot_class_predictions(SPEARobj.ordinal)
```


```{r fig.height = 2.5, fig.width = 10}
k <- 1
subject_ids <- rownames(SPEARobj.ordinal$data$Y)
pred.list <- SPEAR.get_predictions(SPEARobj.ordinal)
probabilities <- pred.list$OrdinalResponse$probabilities
predictions <- pred.list$OrdinalResponse$predictions
subject.probabilities <- as.data.frame(probabilities[rownames(probabilities) %in% subject_ids,])
subject.probabilities <- dplyr::mutate(subject.probabilities, 
                                       PredictedClass = paste0("Class", predictions[names(predictions) %in% subject_ids]),
                                       Actual = paste0("Class", SPEARobj.ordinal$data$Y[rownames(SPEARobj.ordinal$data$Y) %in% subject_ids,k]),
                                       Subject = rownames(subject.probabilities))
df.melt <- reshape2::melt(subject.probabilities, id.vars = c("PredictedClass", "Actual", "Subject"))
ggplot(df.melt) +
  geom_line(aes(x = variable, y = value, group = Subject, color = PredictedClass)) +
  geom_point(aes(x = variable, y = value, group = Subject, color = PredictedClass)) +
  scale_x_discrete(labels = (1:ncol(subject.probabilities)-1)) +
  xlab(NULL) +
  ylab("Probabillity of Class Assignment") +
  scale_color_brewer(palette = "Spectral", direction = 1) +
  scale_fill_brewer(palette = "Spectral", direction = 1) +
  theme_classic() +
  theme(panel.background = element_rect(fill = "black")) +
  facet_wrap(vars(Actual), nrow = 1)
```

