---
title: "SPEAR: Downstream Analysis"
author: "Jeremy Gygi"
date: "3/29/21"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{SPEAR_3-downstream_analysis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

### This vignette will provide insight into downstream analysis using SPEAR:

1. Analyzing all SPEAR models (including SPEARgaussian)

2. Functions specific to SPEARordinal, SPEARbinomial, and SPEARmultinomial models

##### Installing SPEAR:

Follow installation instructions in README (located [here](https://github.com/jgygi/SPEAR))

##### Required Libraries:

```{r message = FALSE, warning = FALSE}
library(SPEAR)
source("../R/SPEARkit3.R")
```

### Overview

![SPEAR overview](vignette_spearoverview.png){width=100%}

### The SPEARobject

```{r}
# Load in the SPEARobject saved from running SPEAR:
file.path <- getwd()
SPEARobj <- readRDS(paste0(file.path, "/SPEAR_vignette_object_gaussian.rds"))
names(SPEARobj)
```

The SPEAR object returned has 4 components:
1. fit (the cross-validated SPEAR object)
2. cv.eval (the results from evaluating the fit object)
3. params (the parameters used for this specific SPEAR object)
4. data (the data used to train the SPEAR models in the object)

*Variables used to reference results:*

+ $N$ - number of subjects [nrow($X$)]

+ $P$ - number of total features (across all omics) [ncol($X$)]

+ $D$ - number of omics datasets [length($X$)]

+ $G$ - number of groups of subjects (typically 1)

+ $Y$ - number of response vars [ncol($Y$)]

+ $K$ - number of factors SPEAR constructed

+ $W$ - number of weights used in CV

+ $F$ - number of folds used in CV



1. fit - the cross-validated SPEAR object

    + results
    
        + post_betas ($P$ x $K$ x $G$ x $W$)
        
        + post_bys ($K$ x $Y$ x $W$)
        
        + post_bxs ($K$ x $P$ x $W$)
        
        + post_pis ($P$ x $K$ x $W$)
        
        + post_selections ($P$ x $K$ x $W$)
        
        + interceptsX ($P$)
        
        + interceptsY ($Y$)
        
    + factors_coefs ($P$ x $K$ x $G$ x $F$ x $W$) # combine by F for out of sample
    
    + projection_coefs ($K$ x $Y$ x $F$ x $W$)    #
    
    + foldid ($N$)
    
2. cv.eval - results from evaluating 'fit'

    + projection_coefs ($K$ x $Y$ x $W$) # after getting coefficients, use all data
    
    + reg_coefs ($P$ x $G$ x $Y$ x $W$)
    
    + intercepts ($Y$)
    
    + cvm ($W$ x $Y$)
    
    + cvsd ($W$ x $Y$)
    
    + factor_contributions ($K$ x $Y$ x $W$)
    
    + Yhat.keep ($N$ x $Y$ x $W$) # cv.predictions
    
3. params - the parameters used to generate 'fit' by SPEAR

    + weights ($W$) - values of '$w$' used in SPEAR
    
    + foldid ($N$) - integer of 1:$F$
    
    + seed
    
    + num_factors
    
    + max_iter
    
    + thres_elbo
    
    + thres_count
    
4. data - the data used by SPEAR

While it isn't necessary to extract these matrices directly from the model, it is useful to understand where the results are coming from. Most typical downstream analyses can be performed using the built-in SPEAR functions described below.

### Weight parameter 'w'

SPEAR generates factors based on a weight parameter, $w$. This parameter helps drive the factor construction process:

![SPEAR overview](vignette_weightexplanation.png){width=100%}

When $w\approx0$, it means that SPEAR will focus on generating factors that explain the response ($Y$). 

When $w\ge1$, it means that SPEAR is equally considering the relationship between the constructed factors and both the datasets ($X$) and the response ($Y$). 

This $w$ parameter is tuned automatically, meaning SPEAR will return factors constructed on the "best" $w$ (based on mean cross-validated prediction error from the training) by default. 

SPEAR will generate a different set of factors for each value supplied in the 'weights' argument (if no argument is supplied, will default to c(2, 1.5, 1.0, 0.5, 0.0).
```{r}
# Check values of 'w' (weights) used by SPEAR:
SPEARobj$params$weights
```

SPEAR uses the mean cross-validation error to determine which is the best $w$ to use.
```{r}
# Get best weights (w):
SPEAR::SPEAR.get_best_weights(SPEARobj, w.method = 'min')
SPEAR::SPEAR.get_best_weights(SPEARobj, w.method = 'sd')
```

Why did SPEAR return a different "best" $w$? Let's plot the mean cross-validation errors to visualize the impact of $w$ on the data.

```{r fig.width = 7, fig.height = 5}
SPEAR::SPEAR.plot_cv_loss(SPEARobj)
```

This function will plot the overall minimum cross validation loss for a SPEAR object (mean cross validation error for `family = "gaussian"`).

The thick dashed line shows the **minimum overall** cross validated error (in this plot it extends from $w = 2.0$).

The thin dashed line shows **1 standard deviation** of cross validated error above the **minimum overall** cross validated error (i.e. one error bar above $w = 1.8$ for this example).

It looks like $w = 2.0$ was barely the lowest overall. However, if we used `w.method = 'sd'` and a lower weight such as $w = 1.0$ were the lowest overall, if a higher weight such as $w = 2.0$ were within 1 cross validation standard deviation, we would choose that weight instead. 

Whenever $w \ge 1$ the results are typically quite similar, so this shouldn't change too much.

### The SPEARmodel

Once we have chosen a value of $w$ to use, we can generate a SPEARmodel:

```{r}
SPEARmodel <- SPEAR::get_SPEAR_model(SPEARobj)
```

The SPEARmodel is similar to a SPEARobject, but it is used for a single $w$ value. Most downstream functions built into the SPEAR package utilize a SPEARmodel as input.

The SPEARmodel can be broken down into the following lists:

+ factors

    + factor.scores
    
        + in.sample
        
        + out.of.sample
    
    + contributions
    
        + X
        
        + Y
        
        + Y.pvals
    
    + features
    
        + [Factor1
        
            + OmicsDataset1
            
                +  features
                
                +  probabilities
                
                +  coefficients
                
            + ...
                
            + [last OmicsDataset]
            
        + ...
        
        + [last Factor]

+ predictions

    +  in.sample
    
    +  out.of.sample

+ fit

    +  post_bxs
    
    +  post_bys
    
    +  post_bys_cv
    
    +  post_betas
    
    +  post_betas_cv
    
    +  post_selections
    
    +  intercepts
    
+ params

    +  w
    
    +  family
    
    +  foldid
    
    +  seed
    
    +  num_factors
    
    +  max_iter
    
    +  thres_elbo
    
    +  thres_count

+ data

    + X
    
    + Y
    
    + xlist

Let's see what value of $w$ our SPEAR model is using:

```{r}
SPEARmodel$params$w
```

There are other ways to specify which value of $w$ a SPEARmodel should use:

```{r eval = FALSE}
### Specifying w.method:
# Get a SPEARmodel for lowest cv error:
SPEARmodel <- SPEAR::get_SPEAR_model(SPEARobj, w.method = "min")
# Get a SPEARmodel for the highest weight within 1 sd of the lowest cv error:
SPEARmodel <- SPEAR::get_SPEAR_model(SPEARobj, w.method = "sd")

### Specifying w:
# Note: if the provided weight is not within the provided weights
#       (SPEARobj$params$weights), it will return the closest weight 
#       to the value provided.
SPEARmodel <- SPEAR::get_SPEAR_model(SPEARobj, w = 0)
SPEARmodel <- SPEAR::get_SPEAR_model(SPEARobj, w = 1)
```

### Analyzing SPEARmodels with different `family` arguments: 

#### SPEARgaussian

1. Load the SPEARobject:

```{r}
file.path <- getwd()
SPEARobj.gaussian <- readRDS(paste0(file.path, "/SPEAR_vignette_object_gaussian.rds"))
```

2. Choose a weight for a SPEARmodel:

```{r fig.width = 7}
# Plot cv loss:
SPEAR::SPEAR.plot_cv_loss(SPEARobj.gaussian)
```

This is the same SPEARobject used above, so we will use the SPEARmodel for $w = 2$:

```{r}
SPEARmodel.w2 <- SPEAR::get_SPEAR_model(SPEARobj.gaussian)
```

For this vignette, we will also make a $w = 0$ model to demonstrate the differences between $w = 0$ and $w \ge 1$.

```{r}
SPEARmodel.w0 <- SPEAR::get_SPEAR_model(SPEARobj.gaussian, w = 0)
```


##### Factor Contributions:

We can see how much is explained from both the $w = 2$ and $w = 0$ SPEARmodels:

```{r fig.width = 7, fig.height = 5, warning=FALSE}
SPEAR::SPEAR.plot_factor_contributions(SPEARmodel.w2)
SPEAR::SPEAR.plot_factor_contributions(SPEARmodel.w0)
```

##### Factor Scores

Because SPEAR is run through cross-validation, you can choose to inspect the `in.sample` factors (those generated by $$all$$ of the training data) as well as the `out.of.sample` factors (each fold uses the factors generated from when it is left out as a validation set). This is done with the `forecast` argument:

```{r fig.width = 7, fig.height = 4}
SPEAR::SPEAR.plot_factor_scores(SPEARmodel.w2, forecast = 'out.of.sample')
```

 Removing the jitter:
 
```{r fig.width = 7, fig.height = 2.5}
SPEAR::SPEAR.plot_factor_scores(SPEARmodel.w2, forecast = 'out.of.sample', jitter.points = FALSE)
```

These plots don't show much on their own. Let's add a `groups` argument. This will color the points by some variable defined in a named vector... We can give each point (a sample) their respective response value as a group:

```{r fig.width = 7, fig.height = 2.5}
custom.groups <- SPEARmodel.w2$data$Y
names(custom.groups) <- rownames(SPEARmodel.w2$data$Y)
SPEAR::SPEAR.plot_factor_scores(SPEARmodel.w2, groups = custom.groups, forecast = 'out.of.sample')
```

It looks like Factors 1 and 2 have strong correlations with the group (we set to Y).

You can set the `groups` parameter to be continuous or categorical. Here we will bin our continuous response into different groups based on rounding them down to their nearest integer:

```{r fig.width = 7, fig.height = 2.5}
custom.groups <- factor(floor(SPEARmodel.w2$data$Y))
names(custom.groups) <- rownames(SPEARmodel.w2$data$Y)
SPEAR::SPEAR.plot_factor_scores(SPEARmodel.w2, groups = custom.groups, forecast = 'out.of.sample')
```

What do the `in.sample` factor scores look like? We hope to not see much difference (this would mean the model overfit to the training data)...

```{r fig.width = 7, fig.height = 2.5}
custom.groups <- factor(floor(SPEARmodel.w2$data$Y))
names(custom.groups) <- rownames(SPEARmodel.w2$data$Y)
SPEAR::SPEAR.plot_factor_scores(SPEARmodel.w2, groups = custom.groups, forecast = 'in.sample')
```

They look very similar. This means the model appears to generalize well to testing (external) data.

##### Feature Selection:

Let's inspect the loadings matrix and the probabilities.

The loadings represent the coefficients used by SPEAR to convert the Omics data (X) into the factor scores...

There are two types of coefficients returned by SPEAR:

  + **regression** - these are the coefficients used to turn $X$ (the Omics datasets) into $U$ (the factor scores). These will be quite sparse, zeroing out redundant signals in the feature set.
  
  + **projection** - these are the coefficients used to turn $U$ (the factor scores) back into $X$ (the Omics datasets). Features that were zeroed out from sparsity in the *regression* coefficients may not be zeroed out in the *projection* coefficients if they are meaningful features.
  
Use the `SPEAR.plot_factor_coefficients` function to plot coefficients (specify the `type` parameter to choose which coefficients to plot).

```{r fig.width = 10, fig.height = 5, fig.align = "center"}
p1 <- SPEAR::SPEAR.plot_factor_coefficients(SPEARmodel, type = "regression")
p2 <- SPEAR::SPEAR.plot_factor_coefficients(SPEARmodel, type = "projection")

cowplot::plot_grid(p1, p2, nrow = 1)
```

Now, let's look at the post selection probabilities of the features...

The probabilities represent the posterior selection probabilities assigned by SPEAR to each feature...

```{r fig.width = 12, fig.height = 5, fig.align = "center"}
p1 <- SPEAR::SPEAR.plot_factor_probabilities(SPEARmodel.w2, type = "marginal") + ggplot2::ggtitle("Marginal Probabilities")
p2 <- SPEAR::SPEAR.plot_factor_probabilities(SPEARmodel.w2, type = "projection") + ggplot2::ggtitle("Projection Probabilities")
p3 <- SPEAR::SPEAR.plot_factor_probabilities(SPEARmodel.w2, type = "joint") + ggplot2::ggtitle("Joint Probabilities", "min(P.marginal, P.projection)")

cowplot::plot_grid(p1, p2, p3, nrow = 1)
```

Again, it looks like OmicsData4 is the dataset that has the most features that are used in our model. 

##### Feature Tables

The `SPEAR.get_feature_table` function is used to return a list of features based on a variety of criteria:

```{r}
# Get features with the highest joint.probability and projection.coefficient for Factor 1 (across all Omics datasets):
feature.table <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "probability", factors = 1)
head(feature.table)
```

By default, when `rank = "probability"` (default), features first must pass the `coefficient.cutoff` (for projection.coefficient) and `probability.cutoff` (for joint.probability) cutoff parameters. By default, these are set to `0` (coefficient.cutoff) and `0.95` (joint.probability). They are finally sorted first by joint.probability (high to low), then by magnitude (absolute value) of the projection.coefficient.

You can update any of the above parameters to get the feature set desired:

```{r}
# Get features with the highest joint.probability and projection.coefficient from the "OmicsData4" dataset (for all 5 factors):
feature.table <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "probability", factors = 1:5, omics = "OmicsData4")
head(feature.table)

# Get features with the highest joint.probability and projection.coefficient from the "OmicsData2" dataset for Factor 2:
# NOTE: There aren't any, so SPEAR will return an empty data.frame
feature.table <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "probability", factors = 2, omics = "OmicsData2")
head(feature.table)
```

You can also ask for correlation coefficients / p.values from correlation tests with the Factor scores with the `correlation.method` parameter:

```{r}
feature.table <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "probability", factors = 1, correlation = "pearson")
head(dplyr::select(feature.table, Feature, Dataset, in.sample.correlation, in.sample.pvalue, out.of.sample.correlation, out.of.sample.pvalue))
```

Finally, if you are interested in getting a subset of features that best explain the most variance, use `rank = "variance"`. This method will first have features checked against `correlation.cutoff` and `probability.cutoff`, take the top feature after ranking by joint.probability and abs(projection.coefficient), regress it with the requested factor scores to calculate variance explained, and repeat this process after regressing out each feature...

By default, `rank = "variance"` will set the parameter `var.cutoff = 0.7`, but this can be overwritten by a new cutoff *or* by using the `num.features` parameter (SPEAR will force itself to find the top `num.features` features using the regressing-out method explained above)

```{r}
feature.table.var <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "variance", factors = 1, num.features = 30)
head(dplyr::select(feature.table.var, Feature, Dataset, var.exp.added, var.exp.total))
```


This function (`SPEAR.get_feature_table`) will return a data.frame table with:

+ **Feature** (the name supplied to SPEAR, taken from colnames(SPEARmodel\$data\$X))

+ **Omic** (which dataset does the feature belong to? Taken from names(SPEARmodel\$data\$xlist))

+ **Factor** (Which factor is being referenced? The same feature may appear with a different **Coefficient**/**Probability** for different **Factor**)

+ **regression.coefficient** (the loading given to the feature for the respective **Factor** for $U \sim X$)

+ **projection.coefficient** (the loading given to the feature for the respective **Factor** for $X \sim U$)

+ **projection.probability** (the posterior selection probability of the projection.coefficients being important for the respective **Factor**)

+ **marginal.probability** (the posterior selection probability that a feature correlates with the respective **Factor** score)

+ **joint.probability** (calculated by $min($ projection.probability, marginal.probability$)$)

To get the columns below, set the `correlation` parameter to `"spearman"`, `"pearson"`, or `"kendall"` (defaults to `"none"`)

+ **in.sample.correlation** / **out.of.sample.correlation** (correlation calculated between respective **Factor** scores and feature values. Correlation method defined by `correlation`)

+ **in.sample.pvalue** / **out.of.sample.pvalue** (pvalue from correlation test calculated between respective **Factor** scores and feature values. Correlation test method defined by `correlation`)

To get the columns below, set the `rank` to `"variance"`:

+ **Rank** (The rank of the feature from the original ranking (by joint.probability and then abs(projection.coefficient)))

+ **var.exp.added** (How much variance of the current **Factor** scores is added when including the current feature?)

+ **var.exp.total** (How much variance of the current **Factor** scores is explained when using all of the features through the current feature?)

Let's get another visual representation of the top features. We can use the `SPEAR.plot_feature_table` function to plot all features (points) by their coefficient (X axis) and posterior selection probability (Y axis). The white bar in the middle represents the `coefficient.cutoff` parameter (defaults to .01), and the Y axis will begin at the `probability.cutoff` (defaults to .5).

```{r, fig.width = 7, fig.height = 6}
SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table, plot.type = "probability")
```

We use the `SPEAR.plot_feature_summary` function to get the actual names and coefficients of the top `max.per.factor` features. Specify which factors are desired with the `factors` argument (defaults to `NULL`, or all factors), set the `probability.cutoff` and `coefficient.cutoff`, and choose which omics to include (with `omics`)...

```{r, fig.width = 7, fig.height = 5}
SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table, plot.type = "summary")
```


##### Prediction:

Predictions for training samples are generated when calling `get_SPEAR_model`. They are stored under `predictions`:

```{r}
names(SPEARmodel.w2$predictions)
head(SPEARmodel.w2$predictions$in.sample)
head(SPEARmodel.w2$predictions$out.of.sample)
```

The `"in.sample"` predictions come from factors built on all of the folds of the data, whereas `"out.of.sample"` predictions come from factors built in a leave-one-out cross-validation.

Use `SPEAR.plot_overfit` to compare the in.sample vs. out.of.sample predictions

```{r fig.width = 4, fig.height = 4, fig.align = "center"}
SPEAR::SPEAR.plot_overfit(SPEARmodel)
```

We can still use the `groups` argument here... let's try coloring by the assigned `foldid` for cross-validation...

```{r fig.width = 5, fig.height = 4, fig.align = "center"}
custom.groups <- factor(SPEARmodel.w2$params$foldid)
names(custom.groups) <- rownames(SPEARmodel.w2$data$Y)
SPEAR::SPEAR.plot_overfit(SPEARmodel, groups = custom.groups)
```

There don't appear to be any obvious patterns amongst the different folds for this SPEARmodel.

You can then plot the SPEAR predictions against the "true" response for the cross-validation training data:

```{r fig.height = 5, fig.width = 6, fig.align="center"}
true.response <- SPEARmodel.w2$data$Y
out.of.sample.preds <- SPEARmodel.w2$predictions$out.of.sample
in.sample.preds <- SPEARmodel.w2$predictions$in.sample

predictions.data.frame <- as.data.frame(cbind(true.response, out.of.sample.preds, in.sample.preds))
colnames(predictions.data.frame) <- c("True", "Out.of.Sample", "In.Sample")
head(predictions.data.frame)

# Out of sample vs. True:
ggplot2::ggplot(data = predictions.data.frame, ggplot2::aes(x = Out.of.Sample, y = True)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(method = "lm") +
  ggplot2::theme_bw()

# In sample vs. True
ggplot2::ggplot(data = predictions.data.frame, ggplot2::aes(x = In.Sample, y = True)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(method = "lm") +
  ggplot2::theme_bw()
```
