---
title: "SPEAR: Downstream Analysis"
author: "Jeremy Gygi"
date: "3/29/21"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{SPEAR_3-downstream_analysis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

### This vignette will provide insight into downstream analysis using SPEAR:

1. Analyzing all SPEAR models

2. Feature Selection (through a feature.table)

##### Installing SPEAR:

Follow installation instructions in README (located [here](https://github.com/jgygi/SPEAR))

##### Required Libraries:

```{r message = FALSE, warning = FALSE}
library(SPEAR)
```

### Overview

![SPEAR overview](vignette_spearoverview.png){width=100%}

### The SPEARobject

```{r}
# Load in the SPEARobject saved from running SPEAR:
file.path <- getwd()
SPEARobj <- readRDS(paste0(file.path, "/SPEAR_vignette_object_gaussian.rds"))
names(SPEARobj)
```

The SPEAR object returned has 4 components:
1. fit (the cross-validated SPEAR object)
2. cv.eval (the results from evaluating the fit object)
3. params (the parameters used for this specific SPEAR object)
4. data (the data used to train the SPEAR models in the object)

*Variables used to reference results:*

+ $N$ - number of subjects [nrow($X$)]

+ $P$ - number of total features (across all omics) [ncol($X$)]

+ $D$ - number of omics datasets [length($X$)]

+ $G$ - number of groups of subjects (typically 1)

+ $Y$ - number of response vars [ncol($Y$)]

+ $K$ - number of factors SPEAR constructed

+ $W$ - number of weights used in CV

+ $F$ - number of folds used in CV



1. fit - the cross-validated SPEAR object

    + results
    
        + post_betas ($P$ x $K$ x $G$ x $W$)
        
        + post_bys ($K$ x $Y$ x $W$)
        
        + post_bxs ($K$ x $P$ x $W$)
        
        + post_pis ($P$ x $K$ x $W$)
        
        + post_selections ($P$ x $K$ x $W$)
        
        + interceptsX ($P$)
        
        + interceptsY ($Y$)
        
    + factors_coefs ($P$ x $K$ x $G$ x $F$ x $W$) # combine by F for out of sample
    
    + projection_coefs ($K$ x $Y$ x $F$ x $W$)    #
    
    + foldid ($N$)
    
2. cv.eval - results from evaluating 'fit'

    + projection_coefs ($K$ x $Y$ x $W$) # after getting coefficients, use all data
    
    + reg_coefs ($P$ x $G$ x $Y$ x $W$)
    
    + intercepts ($Y$)
    
    + cvm ($W$ x $Y$)
    
    + cvsd ($W$ x $Y$)
    
    + factor_contributions ($K$ x $Y$ x $W$)
    
    + Yhat.keep ($N$ x $Y$ x $W$) # cv.predictions
    
3. params - the parameters used to generate 'fit' by SPEAR

    + weights ($W$) - values of '$w$' used in SPEAR
    
    + foldid ($N$) - integer of 1:$F$
    
    + seed
    
    + num_factors
    
    + max_iter
    
    + thres_elbo
    
    + thres_count
    
4. data - the data used by SPEAR

While it isn't necessary to extract these matrices directly from the model, it is useful to understand where the results are coming from. Most typical downstream analyses can be performed using the built-in SPEAR functions described below.

### Weight parameter 'w'

SPEAR generates factors based on a weight parameter, $w$. This parameter helps drive the factor construction process:

![SPEAR overview](vignette_weightexplanation.png){width=100%}

When $w\approx0$, it means that SPEAR will focus on generating factors that explain the response ($Y$). 

When $w\ge1$, it means that SPEAR is equally considering the relationship between the constructed factors and both the datasets ($X$) and the response ($Y$). 

This $w$ parameter is tuned automatically, meaning SPEAR will return factors constructed on the "best" $w$ (based on mean cross-validated prediction error from the training) by default. 

SPEAR will generate a different set of factors for each value supplied in the 'weights' argument (if no argument is supplied, will default to c(2, 1.5, 1.0, 0.5, 0.0).
```{r}
# Check values of 'w' (weights) used by SPEAR:
SPEARobj$params$weights
```

SPEAR uses the mean cross-validation error to determine which is the best $w$ to use.
```{r}
# Get best weights (w):
SPEAR::SPEAR.get_best_weights(SPEARobj, w.method = 'min')
SPEAR::SPEAR.get_best_weights(SPEARobj, w.method = 'sd')
```

Why did SPEAR return a different "best" $w$? Let's plot the mean cross-validation errors to visualize the impact of $w$ on the data.

```{r fig.width = 7, fig.height = 5}
SPEAR::SPEAR.plot_cv_loss(SPEARobj)
```

This function will plot the overall minimum cross validation loss for a SPEAR object (mean cross validation error for `family = "gaussian"`).

The thick dashed line shows the **minimum overall** cross validated error (in this plot it extends from $w = 2.0$).

The thin dashed line shows **1 standard deviation** of cross validated error above the **minimum overall** cross validated error (i.e. one error bar above $w = 1.8$ for this example).

It looks like $w = 2.0$ was barely the lowest overall. However, if we used `w.method = 'sd'` and a lower weight such as $w = 1.0$ were the lowest overall, if a higher weight such as $w = 2.0$ were within 1 cross validation standard deviation, we would choose that weight instead. 

Whenever $w \ge 1$ the results are typically quite similar, so this shouldn't change too much.

### The SPEARmodel

Once we have chosen a value of $w$ to use, we can generate a SPEARmodel:

```{r}
SPEARmodel <- SPEAR::get_SPEAR_model(SPEARobj)
```

The SPEARmodel is similar to a SPEARobject, but it is used for a single $w$ value. Most downstream functions built into the SPEAR package utilize a SPEARmodel as input.

The SPEARmodel can be broken down into the following lists:

+ factors

    + factor.scores
    
        + in.sample
        
        + out.of.sample
    
    + contributions
    
        + X
        
        + Y
        
        + Y.pvals
    
    + features
    
        + [Factor1]
        
            + OmicsDataset1
            
                +  features
                
                +  probabilities
                
                +  coefficients
                
            + ...
                
            + [last OmicsDataset]
            
        + ...
        
        + [last Factor]

+ predictions

    +  in.sample
    
    +  out.of.sample

+ fit

    +  post_bxs
    
    +  post_bys
    
    +  post_bys_cv
    
    +  post_betas
    
    +  post_betas_cv
    
    +  post_selections
    
    +  intercepts
    
+ params

    +  w
    
    +  family
    
    +  foldid
    
    +  seed
    
    +  num_factors
    
    +  max_iter
    
    +  thres_elbo
    
    +  thres_count

+ data

    + X
    
    + Y
    
    + xlist

Let's see what value of $w$ our SPEAR model is using:

```{r}
SPEARmodel$params$w
```

There are other ways to specify which value of $w$ a SPEARmodel should use:

```{r eval = FALSE}
### Specifying w.method:
# Get a SPEARmodel for lowest cv error:
SPEARmodel <- SPEAR::get_SPEAR_model(SPEARobj, w.method = "min")
# Get a SPEARmodel for the highest weight within 1 sd of the lowest cv error:
SPEARmodel <- SPEAR::get_SPEAR_model(SPEARobj, w.method = "sd")

### Specifying w:
# Note: if the provided weight is not within the provided weights
#       (SPEARobj$params$weights), it will return the closest weight 
#       to the value provided.
SPEARmodel <- SPEAR::get_SPEAR_model(SPEARobj, w = 0)
SPEARmodel <- SPEAR::get_SPEAR_model(SPEARobj, w = 1)
```

### Analyzing SPEARmodels with different `family` arguments: 

#### SPEARgaussian

1. Load the SPEARobject:

```{r}
file.path <- getwd()
SPEARobj.gaussian <- readRDS(paste0(file.path, "/SPEAR_vignette_object_gaussian.rds"))
```

2. Choose a weight for a SPEARmodel:

```{r fig.width = 7}
# Plot cv loss:
SPEAR::SPEAR.plot_cv_loss(SPEARobj.gaussian)
```

This is the same SPEARobject used above, so we will use the SPEARmodel for $w = 2$:

```{r}
SPEARmodel.w2 <- SPEAR::get_SPEAR_model(SPEARobj.gaussian)
```

For this vignette, we will also make a $w = 0$ model to demonstrate the differences between $w = 0$ and $w \ge 1$.

```{r}
SPEARmodel.w0 <- SPEAR::get_SPEAR_model(SPEARobj.gaussian, w = 0)
```


##### Factor Contributions:

We can see how much is explained from both the $w = 2$ and $w = 0$ SPEARmodels:

```{r fig.width = 7, fig.height = 5, warning=FALSE}
SPEAR::SPEAR.plot_factor_contributions(SPEARmodel.w2)
SPEAR::SPEAR.plot_factor_contributions(SPEARmodel.w0)
```

##### Factor Scores

Because SPEAR is run through cross-validation, you can choose to inspect the `in.sample` factors (those generated by **all** of the training data) as well as the `out.of.sample` factors (each fold uses the factors generated from when it is left out as a validation set). This is done with the `forecast` argument:

```{r fig.width = 7, fig.height = 4}
SPEAR::SPEAR.plot_factor_scores(SPEARmodel.w2, forecast = 'out.of.sample')
```

 Removing the jitter:
 
```{r fig.width = 7, fig.height = 2.5}
SPEAR::SPEAR.plot_factor_scores(SPEARmodel.w2, forecast = 'out.of.sample', jitter.points = FALSE)
```

These plots don't show much on their own. Let's add a `groups` argument. This will color the points by some variable defined in a named vector... We can give each point (a sample) their respective response value as a group:

```{r fig.width = 7, fig.height = 2.5}
custom.groups <- SPEARmodel.w2$data$Y
names(custom.groups) <- rownames(SPEARmodel.w2$data$Y)
SPEAR::SPEAR.plot_factor_scores(SPEARmodel.w2, groups = custom.groups, forecast = 'out.of.sample')
```

It looks like Factors 1 and 2 have strong correlations with the group (we set to Y).

You can set the `groups` parameter to be continuous or categorical. Here we will bin our continuous response into different groups based on rounding them down to their nearest integer:

```{r fig.width = 7, fig.height = 2.5}
custom.groups <- factor(floor(SPEARmodel.w2$data$Y))
names(custom.groups) <- rownames(SPEARmodel.w2$data$Y)
SPEAR::SPEAR.plot_factor_scores(SPEARmodel.w2, groups = custom.groups, forecast = 'out.of.sample')
```

What do the `in.sample` factor scores look like? We hope to not see much difference (this would mean the model overfit to the training data)...

```{r fig.width = 7, fig.height = 2.5}
custom.groups <- factor(floor(SPEARmodel.w2$data$Y))
names(custom.groups) <- rownames(SPEARmodel.w2$data$Y)
SPEAR::SPEAR.plot_factor_scores(SPEARmodel.w2, groups = custom.groups, forecast = 'in.sample')
```

They look very similar. This means the model appears to generalize well to testing (external) data.

### Feature Selection:

Let's inspect the loadings matrix and the probabilities.

The loadings represent the coefficients used by SPEAR to convert the datasets ($X$) into the factor scores ($U$)...

There are two types of coefficients returned by SPEAR:

  + **regression** - these are the coefficients used to turn $X$ (the datasets) into $U$ (the factor scores). These will be quite sparse, zeroing out redundant signals in the feature set.
  
  + **projection** - these are the coefficients used to turn $U$ (the factor scores) back into $X$ (the datasets). Features that were zeroed out from sparsity in the *regression* coefficients may not be zeroed out in the *projection* coefficients if they are meaningful features.
  
Use the `SPEAR.plot_factor_coefficients` function to plot coefficients (specify the `type` parameter to choose which coefficients to plot).

```{r fig.width = 10, fig.height = 5, fig.align = "center"}
p1 <- SPEAR::SPEAR.plot_factor_coefficients(SPEARmodel, type = "regression")
p2 <- SPEAR::SPEAR.plot_factor_coefficients(SPEARmodel, type = "projection")

cowplot::plot_grid(p1, p2, nrow = 1)
```

Now, let's look at the post selection probabilities of the features...

The probabilities represent the posterior selection probabilities assigned by SPEAR to each feature...

```{r fig.width = 12, fig.height = 5, fig.align = "center"}
p1 <- SPEAR::SPEAR.plot_factor_probabilities(SPEARmodel.w2, type = "marginal") + ggplot2::ggtitle("Marginal Probabilities")
p2 <- SPEAR::SPEAR.plot_factor_probabilities(SPEARmodel.w2, type = "projection") + ggplot2::ggtitle("Projection Probabilities")
p3 <- SPEAR::SPEAR.plot_factor_probabilities(SPEARmodel.w2, type = "joint") + ggplot2::ggtitle("Joint Probabilities", "min(P.marginal, P.projection)")

cowplot::plot_grid(p1, p2, p3, nrow = 1)
```

### Feature Tables

The `SPEAR.get_feature_table` function is used to return a list of features based on a variety of criteria:

```{r}
# Get features with the highest joint.probability and projection.coefficient for Factor 1 (across all Omics datasets):
feature.table <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "probability", factors = 1)
```

This function (`SPEAR.get_feature_table`) will return a data.frame table with:

+ **Feature** (the name supplied to SPEAR, taken from colnames(SPEARmodel\$data\$X))

+ **Dataset** (which dataset does the feature belong to? Taken from names(SPEARmodel\$data\$xlist))

+ **Factor** (Which factor is being referenced? The same feature may appear with a different **Coefficient**/**Probability** for different **Factor**)

+ **regression.coefficient** (the loading given to the feature for the respective **Factor** for $U \sim X$)

+ **projection.coefficient** (the loading given to the feature for the respective **Factor** for $X \sim U$)

+ **projection.probability** (the posterior selection probability of the projection.coefficients being important for the respective **Factor**)

+ **marginal.probability** (the posterior selection probability that a feature correlates with the respective **Factor** score)

+ **joint.probability** (calculated by $min($ projection.probability, marginal.probability$)$)

To get the columns below, set the `correlation` parameter to `"spearman"`, `"pearson"`, or `"kendall"` (defaults to `"none"`)

+ **in.sample.correlation** / **out.of.sample.correlation** (correlation calculated between respective **Factor** scores and feature values. Correlation method defined by `correlation`)

+ **in.sample.pvalue** / **out.of.sample.pvalue** (pvalue from correlation test calculated between respective **Factor** scores and feature values. Correlation test method defined by `correlation`)

To get the columns below, set the `rank` to `"variance"`:

+ **Rank** (The rank of the feature from the original ranking (by joint.probability and then abs(projection.coefficient)))

+ **var.exp.added** (How much variance of the current **Factor** scores is added when including the current feature?)

+ **var.exp.total** (How much variance of the current **Factor** scores is explained when using all of the features through the current feature?)

#### The `rank` parameter:

There are currently **2** ways to construct a feature.table (using the `rank` parameter):

1. `rank = "probability"` (return all of the features that pass a coefficient.cutoff and probability.cutoff (see explanation below))

2. `rank = "variance"` (return a subset of features that best explain a set amount of variance for the current factor scores (see explanation below))

##### `rank = "probability"`

By default, when `rank = "probability"` (default), features first must pass the `coefficient.cutoff` (for projection.coefficient) and `probability.cutoff` (for joint.probability) cutoff parameters. By default, these are set to `0` (coefficient.cutoff) and `0.95` (joint.probability). They are finally sorted first by joint.probability (high to low), then by magnitude (absolute value) of the projection.coefficient.

You can update any of the above parameters to get the feature set desired:

```{r}
# Get features with the highest joint.probability and projection.coefficient from the "OmicsData4" dataset (for all 5 factors):
feature.table <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "probability", factors = 1:5, omics = "OmicsData4")
feature.table[1:10,]

# Get features with the highest joint.probability and projection.coefficient from the "OmicsData2" dataset for Factor 2:
# NOTE: There aren't any, so SPEAR will return an empty data.frame
feature.table <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "probability", factors = 2, omics = "OmicsData2")
feature.table
```

You can also ask for correlation coefficients / p.values from correlation tests with the Factor scores with the `correlation.method` parameter:

```{r}
feature.table <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "probability", factors = 1, correlation = "pearson")
dplyr::select(feature.table, Feature, Dataset, in.sample.correlation, in.sample.pvalue, out.of.sample.correlation, out.of.sample.pvalue)[1:10,]
```

##### `rank = "variance"`

If you are interested in getting a subset of features that best explain the most variance, use `rank = "variance"`. This method will first have features checked against `correlation.cutoff` and `probability.cutoff`, take the top feature after ranking by joint.probability and abs(projection.coefficient), regress it with the requested factor scores to calculate variance explained, and repeat this process after regressing out each feature...

By default, `rank = "variance"` will set the parameter `var.cutoff = 0.7`, but this can be overwritten by a new cutoff *or* by using the `num.features` parameter (SPEAR will force itself to find the top `num.features` features using the regressing-out method explained above)

```{r}
feature.table.var <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "variance", factors = 1, num.features = 30)
dplyr::select(feature.table.var, Feature, Dataset, var.exp.added, var.exp.total)[1:10,]
```

#### Visualizing feature.tables

The function `SPEAR.plot_feature_table` can be used to visualize the subset of features generated by `SPEAR.get_feature_table`.

Different plots can be generated by changing the `plot.type` argument (see examples below). 

**NOTE**: Some `plot.type` requests will not be possible unless a feature.table was constructed in the correct way (i.e. `plot.type == "variance"` requires that one use `SPEAR.get_feature_table(..., rank = "variance")`)...

##### `plot.type = "probability"`

This will plot each feature by its *projection.coefficient* (x-axis) and its *joint.probability* (y-axis), and split by each dataset (vertically) and Factor (horizontally). For definitions of these coefficients/probabilities, see above.

```{r, fig.width = 10, fig.height = 6}
# You can set the coefficient.cutoff and probability.cutoff both to 0 to get all features:
feature.table.all <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "probability", 
                                                    coefficient.cutoff = 0,
                                                    probability.cutoff = 0)
SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.all, plot.type = "probability")
```

```{r, fig.width = 10, fig.height = 6}
# Updating the feature.table will update the plot that is being generated (let's bring the probability.cutoff to 0.5)
feature.table.prob0.5 <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "probability", 
                                                    coefficient.cutoff = 0,
                                                    probability.cutoff = 0.5)
SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.prob0.5, plot.type = "probability")
```

##### `plot.type = "summary"`

This will plot each feature (row in the feature.table) by its *projection.coefficient*. This plot will not sort the rows of the feature.table, so be sure to sort it before plotting (see below)

```{r, fig.width = 6, fig.height = 6, fig.align = "center"}
# randomize the rows of the feature.table
feature.table.random_rows <- feature.table[sample(1:nrow(feature.table), nrow(feature.table), replace = FALSE),]
SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.random_rows, plot.type = "summary")
```

```{r, fig.width = 6, fig.height = 6, fig.align = "center"}
# Sort by Dataset, then projection.coefficient
feature.table.sorted <- dplyr::arrange(feature.table.random_rows, Dataset, projection.coefficient)
SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.sorted, plot.type = "summary")
```

```{r, fig.width = 6, fig.height = 6, fig.align = "center"}
# Sort by joint.probability, then magnitude (absolute value) of the projection.coefficient
feature.table.sorted <- dplyr::arrange(feature.table.random_rows, -joint.probability, -abs(projection.coefficient))
SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.sorted, plot.type = "summary")
```

##### `plot.type = "variance"`

```{r, fig.width = 6, fig.height = 6, fig.align = "center"}
# This plot cannot be used if you use a feature.table without 'rank = "variance"'...
feature.table.all <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "probability", 
                                                    coefficient.cutoff = 0,
                                                    probability.cutoff = 0)

try(SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.all, plot.type = "variance"))
```

```{r, fig.width = 5, fig.height = 4, fig.align = "center"}
# Get as many features as it takes to get to 0.75 (75%) variance explained:
feature.table.var <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "variance", factors = 1, var.cutoff = .75)

SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.var, plot.type = "variance")
```

```{r, fig.width = 8, fig.height = 4, fig.align = "center"}
# Get the top 30 features to explain Factor 1:
feature.table.var <- SPEAR::SPEAR.get_feature_table(SPEARmodel.w2, rank = "variance", factors = 1, num.features = 30)

SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.var, plot.type = "variance")
```

As a note: if the number of requested features (`num.features`) is less than the number of features found using the *coefficient.cutoff* and *probability.cutoff*, all features will end up getting returned, but in the order from the regression process. 

The same can be said if there aren't sufficient features that pass both cutoffs to reach a desired variance explained (`var.cutoff`) such as 0.9. In this case, all features will be returned, but in the order from the regression process.

##### `plot.type = "correlation"`

This will plot each feature (every row in the feature.table) in a correlation matrix against every other feature.

An asterisk ('\*') will be plotted when the correlation test returns a p.value < 0.05...

```{r, fig.width = 6, fig.height = 6, fig.align = "center"}
SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.var, plot.type = "correlation")
```

Use the `correlation.method` parameter to choose which correlation test you would like to use ('pearson' (default), 'spearman', or 'kendall')

```{r, fig.width = 6, fig.height = 6, fig.align = "center"}
SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.var, plot.type = "correlation",
                                correlation.method = "spearman")
```

Return the correlation values themselves:

```{r, fig.width = 8, fig.height = 8, fig.align = "center"}
SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.var, plot.type = "correlation",
                                show.correlation.values = TRUE)
```

By default, features are clustered. To disable this, use `cluster.feature = FALSE`: 

```{r, fig.width = 6, fig.height = 6, fig.align = "center"}
SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.var, plot.type = "correlation",
                                cluster.features = FALSE)
```

If there are too many features, consider using `show.feature.names = FALSE`:

```{r, fig.width = 6, fig.height = 6, fig.align = "center"}
SPEAR::SPEAR.plot_feature_table(SPEARmodel.w2, feature.table = feature.table.all[1:100,], plot.type = "correlation",
                                show.feature.names = FALSE)
```

### Correlation Plots:

Use the `SPEAR.plot_correlation` function to easily plot variables against one-another. To do this, change the `x` and `y` parameters. Current options include:

+ Factors (represented by "Factor1", "Factor2", etc...)

+ Any feature in any of the datasets (represented by a string of the feature name)

```{r fig.width = 5, fig.height = 5, fig.align="center"}
# Plot Factor 1 scores vs. Factor 2 scores:
SPEAR::SPEAR.plot_correlation(SPEARmodel,
                              x = "Factor1",
                              y = "Factor2")
```

```{r fig.width = 5, fig.height = 5, fig.align="center"}
# We can look at the feature.table we generated earlier to find a feature to plot.
# looks like 'OmicsData4_feat194' is positively correlated with Factor 1 score
feature.table[1,]

# Plot Factor 1 scores vs. 'OmicsData4_feat194' values:
SPEAR::SPEAR.plot_correlation(SPEARmodel,
                              x = "Factor1",
                              y = "OmicsData4_feat194")
```

This function also supports a `groups` parameter (see above for how to set this up)

```{r fig.width = 5, fig.height = 5, fig.align="center"}
custom.groups <- factor(floor(SPEARmodel.w2$data$Y))
names(custom.groups) <- rownames(SPEARmodel.w2$data$Y)

# Plot Factor 1 scores vs. 'OmicsData4_feat194' values:
SPEAR::SPEAR.plot_correlation(SPEARmodel,
                              x = "Factor1",
                              y = "OmicsData4_feat194",
                              groups = custom.groups)
```

### Prediction:

Predictions for training samples are generated when calling `get_SPEAR_model`. They are stored under `predictions`:

```{r}
names(SPEARmodel.w2$predictions)
SPEARmodel.w2$predictions$in.sample[1:10,]
SPEARmodel.w2$predictions$out.of.sample[1:10,]
```

The `"in.sample"` predictions come from factors built on all of the folds of the data, whereas `"out.of.sample"` predictions come from factors built in a leave-one-out cross-validation.

Use `SPEAR.plot_overfit` to compare the in.sample vs. out.of.sample predictions

```{r fig.width = 4, fig.height = 4, fig.align = "center"}
SPEAR::SPEAR.plot_overfit(SPEARmodel)
```

We can still use the `groups` argument here... let's try coloring by the assigned `foldid` for cross-validation...

```{r fig.width = 5, fig.height = 4, fig.align = "center"}
custom.groups <- factor(SPEARmodel.w2$params$foldid)
names(custom.groups) <- rownames(SPEARmodel.w2$data$Y)
SPEAR::SPEAR.plot_overfit(SPEARmodel, groups = custom.groups)
```

There don't appear to be any obvious patterns amongst the different folds for this SPEARmodel.

You can then plot the SPEAR predictions against the "true" response for the cross-validation training data:

```{r fig.height = 5, fig.width = 6, fig.align="center"}
true.response <- SPEARmodel.w2$data$Y
out.of.sample.preds <- SPEARmodel.w2$predictions$out.of.sample
in.sample.preds <- SPEARmodel.w2$predictions$in.sample

predictions.data.frame <- as.data.frame(cbind(true.response, out.of.sample.preds, in.sample.preds))
colnames(predictions.data.frame) <- c("True", "Out.of.Sample", "In.Sample")
predictions.data.frame[1:10,]

# Out of sample vs. True:
ggplot2::ggplot(data = predictions.data.frame, ggplot2::aes(x = Out.of.Sample, y = True)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(method = "lm") +
  ggplot2::theme_bw()

# In sample vs. True
ggplot2::ggplot(data = predictions.data.frame, ggplot2::aes(x = In.Sample, y = True)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(method = "lm") +
  ggplot2::theme_bw()
```

#### Using SPEAR to predict novel (testing) data

To generalize SPEAR to predict on new testing data, use the `SPEAR.get_predictions` function (see below). To work, the `Xlist` parameter provided needs to be a list of matrices with the same columns as the training data (see `SPEARobj$data$xlist` to get the original ordering). Otherwise, this data will not work with the trained SPEARmodel. It is easiest if the training/test data are split before training SPEAR so that the column structure is preserved for the list of testing Datasets.

Below, we will plot the results from predicting test data (simulated from the SPEAR_1_simulate_data vignette)...

```{r fig.height = 5, fig.width = 6, fig.align="center"}
data <- readRDS("simulated_gaussian_data.rds")

true.response <- data$data.te$Y
pred.response <- SPEAR::SPEAR.get_predictions(SPEARmodel, Xlist = data$data.te$xlist)

predictions.data.frame <- as.data.frame(cbind(true.response, pred.response))
colnames(predictions.data.frame) <- c("True", "Pred")
predictions.data.frame[1:10,]

# Out of sample vs. True:
ggplot2::ggplot(data = predictions.data.frame, ggplot2::aes(x = Pred, y = True)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(method = "lm") +
  ggplot2::theme_bw()

```
